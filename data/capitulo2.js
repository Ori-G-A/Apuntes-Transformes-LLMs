// data/capitulo2.js
const capitulo2 = {
    id: "2",
    title: "Cap√≠tulo 2: Embeddings",
    parts: [
        {
            id: "2.1",
            title: "2.1 Tokenization",
            subsections: [
                {
                    subid: "2.1.1",
                    subtitle: "Notations",
                    description: "Definiciones fundamentales y notaciones para tokenizaci√≥n de texto.",
                    concepts: [
                        {
                            acronym: "Token",
                            name: "Token",
                            definition: "Unidad arbitrariamente definida de texto. Granularidades: (1) Word-level: 'teddy bear'‚Üí['teddy','bear'], f√°cil interpretar, pocas tokens pero vocabulario grande y no maneja OOV; (2) Subword-level (BPE, WordPiece): 'teddy bear'‚Üí['ted','##dy','bear'], vocabulario reducido, maneja OOV mejor pero implementaci√≥n compleja; (3) Character-level: 'teddy'‚Üí['t','e','d','d','y'], vocabulario muy peque√±o, resuelve OOV pero input muy largo y embeddings poco significativos; (4) Byte-level: usa encoding ASCII/UTF, maneja cualquier lenguaje pero input muy largo y patrones dif√≠ciles de interpretar."
                        },
                        {
                            acronym: "V",
                            name: "Vocabulary",
                            definition: "Conjunto fijo V de tokens predefinidos usado como referencia para convertir texto en tokens: V={token‚ÇÅ, token‚ÇÇ,...,token|V|}. Tama√±o |V| depende de: (1) Granularidad (byte<character<subword<word), (2) Corpus (multiling√ºe>monoling√ºe). Incluye special tokens: [PAD] para rellenar hasta longitud m√°xima n‚Çò‚Çê‚Çì (√∫til para batch computations), [UNK] para entidades no en vocabulario."
                        },
                        {
                            acronym: "T",
                            name: "Tokenizer",
                            definition: "Proceso T que convierte entre texto y tokens. (1) Encode: texto‚Üítokens para que modelo procese, (2) Decode: tokens‚Üítexto para traducir predicciones a lenguaje natural. Tipos: (a) Rule-based (TÃÇ=T): no necesita entrenamiento (word/character-level), no aprovecha patrones de datos; (b) Learned (ùíü‚üπT): necesita entrenamiento, mejores resultados al aprender patrones directamente de datos (BPE, WordPiece, Unigram)."
                        },
                        {
                            acronym: "Subword Design",
                            name: "Subword Tokenization Principles",
                            definition: "Razones por las que subword-level supera a word/character: (1) Leverage root meanings: 'run','runner','running' comparten ra√≠z com√∫n, tokenizaci√≥n debe dividir texto reconociendo estas similitudes; (2) Reduce OOV occurrences: reconocer variaciones peque√±as ('bear'/'bears') minimiza palabras tratadas como desconocidas. Limitaci√≥n de word/character-level: no aprovechan significado sem√°ntico de palabras."
                        },
                        {
                            acronym: "Normalization",
                            name: "Normalization Techniques",
                            definition: "T√©cnicas para estandarizar texto ante inconsistencias: (1) Casing: convertir a lowercase ('Teddy bear'‚Üí'teddy bear') evita variaciones por capitalizaci√≥n; (2) Accents: remover acentos ('knuddelb√§r'‚Üí'knuddelbar'), √∫til para franc√©s/alem√°n/espa√±ol/italiano; (3) Unicode: remover s√≠mbolos fuera de lo com√∫n ('teddy bear ¬©'‚Üí'teddy bear'). Asegura procesamiento uniforme del texto."
                        }
                    ]
                },
                {
                    subid: "2.1.2",
                    subtitle: "Subword Algorithms",
                    description: "Algoritmos m√°s comunes para tokenizaci√≥n a nivel de subword.",
                    concepts: [
                        {
                            acronym: "BPE",
                            name: "Byte-Pair Encoding",
                            definition: "M√©todo de tokenizaci√≥n que construye vocabulario aprendiendo de los pares m√°s comunes en corpus. Training: (1) Initialize: dividir corpus en caracteres, contar ocurrencias‚Üívocabulario inicial V de tama√±o v·µ¢; (2) Add elements: contar frecuencia de pares, seleccionar par m√°s frecuente, fusionarlos y agregar a V, actualizar splits en corpus; (3) Finalize: repetir hasta |V|=vf>v·µ¢, agregar tokens especiales como [UNK]. Encoding: dividir texto en caracteres, aplicar reglas de fusi√≥n sucesivamente en orden de inserci√≥n a V. Aplicaciones: GPT, LLaMA y mayor√≠a de LLMs recientes."
                        },
                        {
                            acronym: "WordPiece",
                            name: "WordPiece Algorithm",
                            definition: "Variaci√≥n de BPE que hace fusiones bas√°ndose en el par m√°s probable (likelihood-based) en vez del m√°s frecuente. Usado como tokenizer del modelo BERT. Mantiene estructura similar a BPE pero criterio de fusi√≥n diferente: selecciona pares que maximizan probabilidad en lugar de frecuencia absoluta."
                        },
                        {
                            acronym: "Unigram",
                            name: "Unigram Algorithm",
                            definition: "M√©todo de tokenizaci√≥n subword que asume probabilidad de aparici√≥n de token es independiente de tokens previos. Training: (1) Initialize: comenzar con vocabulario arbitrariamente grande V de todos subconjuntos posibles de caracteres en corpus; (2) Refine: reducir elementos hasta tama√±o deseado‚Üícompute probability usando EM algorithm (‚Ñí(V\\token)-‚Ñí(V)), prune vocabulary manteniendo top 80% de subwords que m√°s aumentan loss (son necesarios), mantener siempre caracteres individuales. Encoding: (1) considerar todas segmentaciones posibles de palabra usando elementos de V, (2) computar probabilidad de cada segmentaci√≥n P(seg)=P(token‚ÇÅ)√óP(token‚ÇÇ)√ó..., (3) seleccionar segmentaci√≥n con score m√°s alto. Aplicaciones: modelo T5."
                        }
                    ]
                }
            ]
        },
        {
            id: "2.2",
            title: "2.2 Token Embeddings",
            description: "M√©todos para encontrar representaci√≥n vectorial de cada token del vocabulario.",
            subsections: [
                {
                    subid: "2.2.1",
                    subtitle: "One-hot Encodings",
                    description: "Tipo est√°ndar y b√°sico de encoding para tokens.",
                    concepts: [
                        {
                            acronym: "OHE",
                            name: "One-Hot Encoding",
                            definition: "M√©todo que representa cada token i de vocabulario V con vector de tama√±o |V| donde elemento i-√©simo es 1 y resto son 0: token·µ¢‚Üí[0,...,0,1,0,...,0]. Limitaciones cr√≠ticas: (1) Token similarity: vectores resultantes son ortogonales sin importar cercan√≠a sem√°ntica (sin√≥nimos deber√≠an estar cerca, ant√≥nimos lejos, pero OHE no captura esto); (2) Dimensionality: dimensi√≥n del vector es tan grande como |V| ~10‚Å¥-10‚Åµ, aumenta significativamente requerimientos computacionales y uso de memoria (|V|‚â´1). No es m√©todo efectivo para NLP moderno."
                        }
                    ]
                },
                {
                    subid: "2.2.2",
                    subtitle: "Continuous Encodings",
                    description: "M√©todos para incorporar significado sem√°ntico dentro de embeddings.",
                    concepts: [
                        {
                            acronym: "Word2vec",
                            name: "Word2vec",
                            definition: "Familia de m√©todos que genera token embeddings que son: (1) Continuous: cada dimensi√≥n es float; (2) Fixed dimension: dimensi√≥n d‚Çò‚Çíd‚Çë‚Çó fija (~10¬≤-10¬≥) independiente de |V|, comprime informaci√≥n y reduce complejidad computacional. Arquitectura: red neuronal shallow con una capa oculta (|V|√ód‚Çò‚Çíd‚Çë‚Çó√ó|V|), capas Projection y Prediction. Despu√©s de entrenar, extraer embeddings x‚Çë‚Çòb‚Çëd aprendidos de capa projection. Resultados: embeddings capturan relaciones intuitivas entre tokens mediante aritm√©tica vectorial (ej: teddy bear-toy+plastic‚âàcotton)."
                        },
                        {
                            acronym: "CBOW",
                            name: "Continuous Bag Of Words",
                            definition: "Modelo word2vec que predice token target basado en promedio de embeddings de tokens de contexto. Idea: entrenar modelo que predice token target dado tokens circundantes (contexto C en cada lado), extraer embeddings aprendidos. Training: (1) Initialize network con hidden layer, fijar context size C; (2) Represent input: considerar OHEs de todos tokens dentro de ventana de contexto; (3) Forward pass: proyectar OHEs de contexto y promediarlos, pasar embedding resultante por output layer para predecir target‚Üí‚Ñí. Extraer embeddings de projection layer. Ventaja: tiempo de entrenamiento relativamente r√°pido (genera una predicci√≥n por token target)."
                        },
                        {
                            acronym: "Skip-gram",
                            name: "Skip-gram",
                            definition: "Modelo word2vec que predice tokens de contexto dentro de ventana fija dado el token target. Idea: entrenar modelo que predice tokens de contexto dado target, extraer embeddings aprendidos. Training: (1) Initialize network con hidden layer, fijar context size C; (2) Represent input: considerar OHE del token target; (3) Forward pass: despu√©s de proyectar OHE del target, predecir cada uno de sus tokens de contexto (2√óC predicciones)‚Üím√∫ltiples ‚Ñí. Extraer embeddings de projection layer. Desventaja: entrenamiento m√°s lento que CBOW porque cada target genera 2√óC data points, aumentando tama√±o de entrenamiento."
                        },
                        {
                            acronym: "Negative Sampling",
                            name: "Negative Sampling",
                            definition: "T√©cnica que simplifica funci√≥n objetivo y reduce complejidad computacional al samplear peque√±o n√∫mero de ejemplos negativos en vez de considerar todos. Problema: en clasificaci√≥n multi-clase, loss usa softmax que requiere sumar sobre todas clases V, muy costoso si |V| grande. Soluci√≥n: reformular como clasificaci√≥n binaria evitando suma costosa. Algorithm: dado target w‚Çú con vecino w‚Çö, (1) Identify positive: par (w‚Çö,w‚Çú) es ejemplo positivo; (2) Sample negatives: samplear conjunto de O(10) observaciones {w‚Çô‚ààN} donde w‚Çô no es vecino de w‚Çú‚Üí(w‚Çô,w‚Çú) ejemplos negativos; (3) Optimize: tratar como clasificaci√≥n binaria‚ÜíL=-log(œÉ(w‚Çö¬∑w‚Çú))-Œ£w‚Çô‚ààN log(œÉ(-w‚Çô¬∑w‚Çú)). Beneficio: solo sumar sobre N‚â™|V|, reducci√≥n significativa en tiempo con buena aproximaci√≥n."
                        },
                        {
                            acronym: "GloVe",
                            name: "Global Vectors",
                            definition: "T√©cnica que aprovecha co-ocurrencias para derivar word embeddings. Idea: construir embeddings basados en informaci√≥n estad√≠stica de matriz de co-ocurrencia que cuantifica qu√© tan seguido aparecen pares de palabras juntas. Algorithm: (1) Construct co-occurrence matrix X: inicializar context window size, construir matriz donde X·µ¢‚±º=n√∫mero de veces que target word i ocurri√≥ con context word j (matriz sim√©trica); (2) Model co-occurrence: modelar log(X·µ¢‚±º) usando target embedding w‚Çú,·µ¢, context embedding wc,‚±º y bias terms: log(X·µ¢‚±º)‚âàw‚Çú,·µ¢·µÄwc,‚±º+b‚Çú,·µ¢+bc,‚±º; (3) Learn weights: gradient descent para minimizar weighted squared loss L=Œ£·µ¢ Œ£‚±º f(X·µ¢‚±º)[log(X·µ¢‚±º)ÃÇ-log(X·µ¢‚±º)]¬≤ donde f es funci√≥n de ponderaci√≥n f(X·µ¢‚±º)=(X·µ¢‚±º/X‚Çò‚Çê‚Çì)^Œ± si X·µ¢‚±º<X‚Çò‚Çê‚Çì, 1 otherwise; (4) Deduce embeddings: dado simetr√≠a, embedding final w=(w‚Çú+wc)/2."
                        }
                    ]
                }
            ]
        },
        {
            id: "2.3",
            title: "2.3 Document Embeddings",
            description: "M√©todos para codificar significado de sentencia aprovechando informaci√≥n de sus tokens.",
            subsections: [
                {
                    subid: "2.3.1",
                    subtitle: "Heuristic Methods",
                    description: "M√©todos basados en reglas que funcionan bien a pesar de su simplicidad.",
                    concepts: [
                        {
                            acronym: "BOW",
                            name: "Bag Of Words",
                            definition: "M√©todo que genera embeddings a nivel de documento considerando frecuencia de aparici√≥n de cada token, bas√°ndose en representaciones OHE. Proceso: (1) Construct vocabulary V con todos tokens del documento; (2) Count occurrence: asociar a cada palabra n√∫mero de veces que apareci√≥; (3) Build document vector: representar documento como vector de dimensi√≥n |V|, suma de frecuencias de palabras asociadas con sus OHE representations. Limitaci√≥n principal: orden de tokens no importa en representaci√≥n vectorial (frases con mismas palabras en diferente orden tienen mismo xBOW)."
                        },
                        {
                            acronym: "n-gram",
                            name: "n-gram Model",
                            definition: "Extensi√≥n de BOW que se basa en frecuencia de secuencias de n palabras consecutivas. Par√°metro n‚â•1 con trade-off: (1) n m√°s alto‚Üíconsidera m√°s palabras co-localizadas pero frecuencias m√°s sparse y mayor costo computacional; (2) n m√°s bajo‚Üífrecuencias menos sparse y menor costo pero pierde contexto de co-localizaci√≥n. Nota: BOW es caso especial de n-gram con n=1."
                        },
                        {
                            acronym: "TF-IDF",
                            name: "Term Frequency-Inverse Document Frequency",
                            definition: "M√©todo que considera tanto frecuencia de palabra en documento dado como su prevalencia en todos los documentos. Objetivo: filtrar palabras muy comunes (ej: 'the') y enfocarse en palabras m√°s importantes. Factores: (1) Term frequency TF(t,d)=f‚Çú,d/Œ£‚Çú'‚ààd f‚Çú',d: ratio de veces que t√©rmino t aparece en documento d sobre conteo de todos t√©rminos en d, normalizado por total de palabras en documento; (2) Inverse document frequency IDF(t,ùíü)=log(Nùíü/N‚Çú,ùíü): funci√≥n de total de documentos Nùíü y documentos N‚Çú,ùíü donde aparece t‚Üímientras m√°s aparece t en documentos, m√°s probable es palabra com√∫n y menos importante. TF-IDF(t,d,ùíü)=TF(t,d)√óIDF(t,ùíü)."
                        }
                    ]
                },
                {
                    subid: "2.3.2",
                    subtitle: "Recurrent Neural Networks",
                    description: "Clase de modelos que fueron state-of-the-art por su desempe√±o en rango amplio de tareas NLP.",
                    concepts: [
                        {
                            acronym: "RNN",
                            name: "Recurrent Neural Network",
                            definition: "Tipo de red neuronal que mantiene hidden state auto-mutante para procesar inputs temporales. Arquitectura: considera inputs x‚ÅΩ·µó‚Åæ y mantiene hidden states h‚ÅΩ·µó‚Åæ. Para cada timestep t: h‚ÅΩ·µó‚Åæ=A‚ÇÅ(W‚Çï‚Çïh‚ÅΩ·µó‚Åª¬π‚Åæ+W‚Çï‚Çìx‚ÅΩ·µó‚Åæ+b‚Çï) y ≈∑‚ÅΩ·µó‚Åæ=A‚ÇÇ(W·µß‚Çïh‚ÅΩ·µó‚Åæ+b·µß) donde W‚Çï‚Çï,W‚Çï‚Çì,W·µß‚Çï,b‚Çï,b·µß son coeficientes compartidos temporalmente, A‚ÇÅ,A‚ÇÇ funciones de activaci√≥n. Training: loss L sobre todos time steps T·µß: L(≈∑,y)=Œ£‚ÇúT·µß L(≈∑‚ÅΩ·µó‚Åæ,y‚ÅΩ·µó‚Åæ). Backpropagation temporal en cada punto en tiempo: ‚àÇL‚ÅΩ·µÄ‚Åæ/‚àÇW=Œ£‚Çú·µÄ ‚àÇL‚ÅΩ·µÄ‚Åæ/‚àÇW|‚ÅΩ·µó‚Åæ. Problema: dificultad capturando dependencias long-term por vanishing gradient (gradientes muy peque√±os durante backpropagation through time)."
                        },
                        {
                            acronym: "RNN Applications",
                            name: "RNN Application Cases",
                            definition: "RNNs se usan en diferentes aplicaciones cambiando input length T‚Çì y output length T·µß: (1) One-to-one (T‚Çì=1,T·µß=1): red neuronal tradicional; (2) One-to-many (T‚Çì=1,T·µß>1): generaci√≥n de texto/m√∫sica, input palabra‚Üíoutput resto de sentencia; (3) Many-to-one (T‚Çì>1,T·µß=1): clasificaci√≥n de sentimiento, input sentencia‚Üíoutput positivo/negativo; (4) Many-to-many (T‚Çì=T·µß): name entity recognition, input sentencia‚Üíoutput clasificaci√≥n word-level; (5) Many-to-many (T‚Çì‚â†T·µß): traducci√≥n autom√°tica, input sentencia en idioma origen‚Üíoutput sentencia en idioma destino."
                        },
                        {
                            acronym: "Gate",
                            name: "Gate Mechanism",
                            definition: "Mecanismo para mitigar vanishing gradient problem con gates que tienen prop√≥sito bien definido. Output de gate G: ŒìG(h‚ÅΩ·µó‚Åª¬π‚Åæ,x‚ÅΩ·µó‚Åæ)=œÉ(WG[h‚ÅΩ·µó‚Åª¬π‚Åæ,x‚ÅΩ·µó‚Åæ]+bG) donde ŒìG‚àà[0,1], WG matriz y bG bias espec√≠ficos de gate G, [h‚ÅΩ·µó‚Åª¬π‚Åæ,x‚ÅΩ·µó‚Åæ] concatenaci√≥n de hidden state e input. Tipos: (1) Input/Update gate Œì·µ¢: filtrar informaci√≥n √∫til del input (Œì·µ¢‚Üí0 ignorar nueva info, Œì·µ¢‚Üí1 incluir nueva info), usado en GRU/LSTM; (2) Forget/Reset gate Œìf: decidir qu√© informaci√≥n descartar (Œìf‚Üí0 olvidar info, Œìf‚Üí1 mantener info), usado en GRU/LSTM; (3) Output gate Œì‚Çí: decidir siguiente hidden state (Œì‚Çí‚Üí0 no output de estado actual, Œì‚Çí‚Üí1 output todo de estado actual), usado en LSTM."
                        },
                        {
                            acronym: "GRU",
                            name: "Gated Recurrent Unit",
                            definition: "Arquitectura basada en RNN que deja fluir informaci√≥n √∫til temporalmente mediante gates especiales. Mantiene cell state c‚ÅΩ·µó‚Åæ adem√°s de hidden state h‚ÅΩ·µó‚Åæ. Usa 2 gates (Œì·µ¢,Œìf): hidden state previo h‚ÅΩ·µó‚Åª¬π‚Åæ se actualiza y fusiona con cell state previo v√≠a input gate Œì·µ¢, partes relevantes de h‚ÅΩ·µó‚Åª¬π‚Åæ interact√∫an con input actual x‚ÅΩ·µó‚Åæ y se guardan en updated cell state cÃÉ‚ÅΩ·µó‚Åæ. Ecuaciones: cÃÉ‚ÅΩ·µó‚Åæ=tanh(Wc[Œìf‚äôh‚ÅΩ·µó‚Åª¬π‚Åæ,x‚ÅΩ·µó‚Åæ]+bc) potential new cell state; c‚ÅΩ·µó‚Åæ=Œì·µ¢‚äôcÃÉ‚ÅΩ·µó‚Åæ+(1-Œì·µ¢)‚äôc‚ÅΩ·µó‚Åª¬π‚Åæ combinaci√≥n de estado previo ponderado por cu√°nto olvidar y cell candidate ponderado por cu√°nto mantener; h‚ÅΩ·µó‚Åæ=c‚ÅΩ·µó‚Åæ cell state y hidden state son iguales en GRU. Beneficio: mitiga vanishing gradient manteniendo informaci√≥n fluyendo y olvidando partes relevantes con complejidad computacional relativamente simple. Existen m√∫ltiples variantes GRU."
                        },
                        {
                            acronym: "LSTM",
                            name: "Long-Short Term Memory",
                            definition: "Generalizaci√≥n de arquitectura GRU que agrega gates para ayudar al modelo a recordar informaci√≥n del pasado y olvidar partes no relevantes. Usa 3 gates (Œì·µ¢,Œìf,Œì‚Çí). Ecuaciones: cÃÉ‚ÅΩ·µó‚Åæ=tanh(Wc[h‚ÅΩ·µó‚Åª¬π‚Åæ,x‚ÅΩ·µó‚Åæ]+bc) potential new cell state usando hidden state previo y input actual; c‚ÅΩ·µó‚Åæ=Œìf‚äôc‚ÅΩ·µó‚Åª¬π‚Åæ+Œì·µ¢‚äôcÃÉ‚ÅΩ·µó‚Åæ combinaci√≥n de estado previo ponderado por Œìf (cu√°nto olvidar) y cell candidate ponderado por Œì·µ¢ (cu√°nto mantener); h‚ÅΩ·µó‚Åæ=Œì‚Çí‚äôtanh(c‚ÅΩ·µó‚Åæ) output gate Œì‚Çí deja fluir informaci√≥n necesaria aplicado a updated cell state v√≠a tanh. Ventajas: retener informaci√≥n por periodo m√°s largo gracias a gates especiales. Desventajas: entrenamiento m√°s largo por complejidad agregada. Existen varias variaciones LSTM."
                        },
                        {
                            acronym: "ELMo",
                            name: "Embeddings from Language Models",
                            definition: "Arquitectura usando bidirectional LSTMs que produce word embeddings context-aware, se basa en informaci√≥n character-level haci√©ndola robusta a palabras OOV. Arquitectura: stack de L capas bidirectional LSTM, permite que embedding de cada token sea funci√≥n de tokens tanto de izquierda como derecha. Proceso: (1) Tokenize input: dividir en caracteres con dimensi√≥n dch‚Çê·µ£, aplicar convolutions para representaciones n-gram character por palabra‚Üídwo·µ£d; (2) Compute hidden representation: para token en posici√≥n t en capa l‚àà[[1,L]], considerar left-to-right hidden state h‚Éó‚Çú,‚Çó (considera tokens 1,...,t-1) y right-to-left hidden state h‚Éñ‚Çú,‚Çó (considera tokens n,...,t+1); (3) Compute final embedding: concatenar ambos hidden states de √∫ltima capa ≈∑‚Çú=[h‚Éó‚Çú,L,h‚Éñ‚Çú,L]. Embedding ELMo de token es funci√≥n de sentencia en que est√°, permitiendo que contexto relevante sea parte de √©l. Training: (1) General pretraining: entrenar modelo self-supervised en set grande de datos; (2) Task-specific finetuning: usar combinaci√≥n lineal de hidden states ≈∑‚Çú·µó·µÉÀ¢·µè=Œ≥·µó·µÉÀ¢·µè Œ£‚ÇóL Œ±‚Çó·µó·µÉÀ¢·µè[h‚Éó‚Çú,L,h‚Éñ‚Çú,L] donde Œ≥·µó·µÉÀ¢·µè y Œ±‚Çó·µó·µÉÀ¢·µè son par√°metros task-specific y entrenables. Naturaleza bidireccional permite a capas ocultas considerar informaci√≥n de todas partes del input."
                        }
                    ]
                },
                {
                    subid: "2.3.3",
                    subtitle: "Attention-based Methods",
                    description: "Concepto de attention que permite al modelo enfocarse directamente en tokens pasados en vez de depender solo de estado auto-mutante √∫nico.",
                    concepts: [
                        {
                            acronym: "Attention Motivation",
                            name: "Motivation for Attention",
                            definition: "LSTMs y GRUs intentan resolver vanishing gradient con gating mechanisms, pero m√©todos imperfectos en pr√°ctica. En traducci√≥n autom√°tica donde traducimos sentencia de longitud T‚Çì, mantener seguimiento de esos tokens pasados juega rol crucial. Idea: introducir grados adicionales de libertad inyectando contexto que es funci√≥n directa de tokens pasados, en vez de depender solo de hidden state que se va mutando."
                        },
                        {
                            acronym: "Attention Weights",
                            name: "Attention-based Weights",
                            definition: "Para token en posici√≥n i, introducir contexto c·µ¢ que contiene informaci√≥n pasada a inyectar, funci√≥n expl√≠cita de tokens pasados en posiciones j‚àà[[1,T‚Çì]] v√≠a su hidden state h‚±º: c·µ¢=Œ£‚±º·µÄÀ£ Œ±·µ¢,‚±ºh‚±º donde Œ±·µ¢,‚±º representa cu√°nto token en posici√≥n i debe prestar atenci√≥n a token pasado en posici√≥n j. Para interpretar Œ±·µ¢,‚±º como probabilidad (Œ£‚±º·µÄÀ£Œ±·µ¢,‚±º=1), representar como resultado de softmax: Œ±·µ¢,‚±º=exp(e·µ¢,‚±º)/Œ£‚±º·µÄÀ£Exp(e·µ¢,‚±º). T√©rmino e·µ¢,‚±º es output de alignment model A que es funci√≥n de hidden state h·µ¢‚Çã‚ÇÅ que se est√° inputeando en posici√≥n i y hidden state h‚±º de token pasado j‚àà[[1,T‚Çì]]."
                        }
                    ]
                }
            ]
        },
        {
            id: "2.4",
            title: "2.4 Embedding Operations",
            description: "Operaciones principales que se pueden realizar sobre embeddings.",
            subsections: [
                {
                    subid: "2.4.1",
                    subtitle: "Similarity",
                    description: "M√©todos para cuantificar similitud entre embeddings.",
                    concepts: [
                        {
                            acronym: "Vector Norm",
                            name: "Vector Norm",
                            definition: "N√∫mero cuantificado de medida de vector x‚ààR‚Åø. Tipos: (1) L‚ÇÅ Manhattan ||x||‚ÇÅ=|x‚ÇÅ|+...+|x‚Çô|, robusto a outliers, promueve sparsity (LASSO) pero no diferenciable en todas partes y poco intuitivo; (2) L‚ÇÇ Euclidean ||x||‚ÇÇ=‚àö(x‚ÇÅ¬≤+...+x‚Çô¬≤), intuitivo y diferenciable pero sensible a outliers y computacionalmente costoso cuando n alto; (3) L‚Çö General ||x||‚Çö=(x‚ÇÅ·µñ+...+x‚Çô·µñ)^(1/p), flexible y customizable ajustando p pero dif√≠cil elegir p √≥ptimo y complejidad computacional agregada especialmente si p‚àâ‚Ñï*; (4) L‚àû Maximum ||x||‚àû=max(x‚ÇÅ,...,x‚Çô), simple de computar y √∫til para bounding constraints pero sensible a outliers. Por default usar L‚ÇÇ norm y notar ||.||=||.||‚ÇÇ."
                        },
                        {
                            acronym: "Cosine Similarity",
                            name: "Cosine Similarity",
                            definition: "Medida de similitud entre dos tokens t‚ÇÅ y t‚ÇÇ que considera √°ngulo Œ∏ formado por sus representaciones vectoriales asociadas: similarity=(t‚ÇÅ¬∑t‚ÇÇ)/(||t‚ÇÅ||||t‚ÇÇ||)=cos(Œ∏). Valores cercanos a 1 indican alta similitud (√°ngulo peque√±o), valores cercanos a 0 indican baja similitud (√°ngulo ~90¬∞), valores cercanos a -1 indican oposici√≥n (√°ngulo ~180¬∞). √ötil porque normaliza por magnitud de vectores, enfoc√°ndose solo en direcci√≥n."
                        }
                    ]
                },
                {
                    subid: "2.4.2",
                    subtitle: "Dimension Reduction",
                    description: "M√©todos para visualizar embeddings de alta dimensi√≥n en un espacio de menor dimensi√≥n.",
                    concepts: [
                        {
                            acronym: "t-SNE",
                            name: "t-distributed Stochastic Neighbor Embedding",
                            definition: "T√©cnica de reducci√≥n de dimensionalidad no lineal que busca preservar la estructura local de los datos. Es particularmente √∫til para la visualizaci√≥n de clusters en 2D o 3D, ya que mantiene los puntos similares cercanos entre s√≠ en el espacio reducido, aunque no preserva bien las distancias globales."
                        },
                        {
                            acronym: "PCA",
                            name: "Principal Component Analysis",
                            definition: "M√©todo lineal que transforma los datos a un nuevo sistema de coordenadas (componentes principales) de tal manera que la mayor varianza posible se proyecte en los primeros ejes. A diferencia de t-SNE, es determinista y busca preservar la estructura global y la dispersi√≥n de los datos."
                        }
                    ]
                },
                {
                    subid: "2.4.3",
                    subtitle: "Fast retrieval",
                    description: "T√©cnicas de b√∫squeda aproximada para encontrar vectores similares de manera eficiente en grandes vol√∫menes de datos.",
                    concepts: [
                        {
                            acronym: "ANN",
                            name: "Approximate Nearest Neighbor",
                            definition: "Categor√≠a de algoritmos que permiten encontrar elementos cercanos en un espacio vectorial de forma r√°pida, sacrificando la precisi√≥n exacta por la velocidad de c√≥mputo. Incluye variantes como LSH, √°rboles KD y escaneo lineal[cite: 196]."
                        },
                        {
                            acronym: "LSH",
                            name: "Locality-Sensitive Hashing",
                            definition: "Algoritmo ANN que proyecta vectores aleatoriamente en cubetas ('buckets'). La idea es que los vectores similares tengan una alta probabilidad de colisionar en la misma cubeta[cite: 193]. Se compone de: (1) Proyecci√≥n aleatoria de los n elementos en 2·¥∑ cubetas; (2) Verificaci√≥n dentro de la misma cubeta comparando solo ese subconjunto peque√±o para confirmar la similitud."
                        }
                    ]
                }
            ]
        }
    ]
};